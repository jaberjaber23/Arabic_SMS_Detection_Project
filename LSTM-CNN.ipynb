{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymystem3\n",
    "#!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f663d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['sms'] = df['sms'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D\n",
    "from keras.layers import LSTM,Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.utils import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 10000\n",
    "X = df['sms']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(\"Shape of train data:\", X_train.shape)\n",
    "print(\"Shape of Test data:\", X_test.shape)\n",
    "print(\"Shape of CV data:\", X_cv.shape)\n",
    "\n",
    "# tokenize input sequences\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_cv = tokenizer.texts_to_sequences(X_cv)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 150\n",
    "# max_review_length = 1000\n",
    "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
    "X_cv = pad_sequences(X_cv, maxlen=max_review_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a794ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train data:\", X_train.shape)\n",
    "print(\"Shape of Test data:\", X_test.shape)\n",
    "print(\"Shape of CV data:\", X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3824074",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "filepath=\"weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_data=(X_cv,y_cv))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ee5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Conv1D, MaxPooling1D, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "filepath = \"weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=256, verbose=1, callbacks=callbacks_list, validation_data=(X_cv, y_cv))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c352858",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''num_max = 1000\n",
    "\n",
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model\n",
    "\n",
    "def check_model(model,x,y):\n",
    "    model.fit(x,y,batch_size=32,epochs=10,verbose=1,validation_split=0.2)\n",
    "\n",
    "m = get_simple_model()\n",
    "check_model(m,X_train,y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f54757",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max_len = 1000\n",
    "def get_cnn_model_v3():    # added filter\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m = get_cnn_model_v3()\n",
    "check_model(m,X_train,y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Conv1D, MaxPooling1D, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f70e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define model architecture\n",
    "embedding_vecor_length = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Define checkpoint to save model weights with best validation accuracy\n",
    "filepath = \"weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=256, verbose=1, callbacks=callbacks_list, validation_data=(X_cv, y_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.load_weights(\"weights_best.hdf5\")\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "scores = model.evaluate(X_test, y_test, verbose=1,batch_size = 256)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.load_weights(\"weights_best.hdf5\")\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "scores = model.evaluate(X_test, y_test, verbose=1,batch_size = 256)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "filepath=\"weights_best_cnn.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_data=(X_cv,y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a224602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.load_weights(\"weights_best_cnn.hdf5\")\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780baf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = ['Model', 'Accuracy']\n",
    "table.add_row(['LSTM', 0])\n",
    "table.add_row(['CNN using LSTM', 0])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48780d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
